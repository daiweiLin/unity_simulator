{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# source:\n",
    "# https://github.com/theRealSuperMario/supermariopy/blob/master/scripts/tflogs2pandas.py\n",
    "#########################################\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import click\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Extraction function\n",
    "def tflog2pandas(path: str) -> pd.DataFrame:\n",
    "    \"\"\"convert single tensorflow log file to pandas DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to tensorflow log file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        converted dataframe\n",
    "    \"\"\"\n",
    "    DEFAULT_SIZE_GUIDANCE = {\n",
    "        \"compressedHistograms\": 1,\n",
    "        \"images\": 1,\n",
    "        \"scalars\": 0,  # 0 means load all\n",
    "        \"histograms\": 1,\n",
    "    }\n",
    "    runlog_data = pd.DataFrame()\n",
    "    try:\n",
    "        event_acc = EventAccumulator(path, DEFAULT_SIZE_GUIDANCE)\n",
    "        event_acc.Reload()\n",
    "        tags = event_acc.Tags()[\"scalars\"]\n",
    "        for tag in tags:\n",
    "            if tag == \"rollout/return\" or tag == \"rollout/Q_mean\": #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Made change here\n",
    "                event_list = event_acc.Scalars(tag)\n",
    "                values = list(map(lambda x: x.value, event_list))\n",
    "                step = list(map(lambda x: x.step, event_list))\n",
    "                r = pd.Series(data=values, index=step, name=tag)\n",
    "                runlog_data = pd.concat([runlog_data, r], axis=1)\n",
    "    # Dirty catch of DataLossError\n",
    "    except:\n",
    "        print(\"Event file possibly corrupt: {}\".format(path))\n",
    "        traceback.print_exc()\n",
    "    return runlog_data\n",
    "\n",
    "\n",
    "def many_logs2pandas(event_paths):\n",
    "    all_logs = pd.DataFrame()\n",
    "    for path in event_paths:\n",
    "        log = tflog2pandas(path)\n",
    "        if log is not None:\n",
    "            if all_logs.shape[0] == 0:\n",
    "                all_logs = log\n",
    "            else:\n",
    "                all_logs = all_logs.append(log, ignore_index=True)\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "def extract_tf_to_csv(logfile: str, write_pkl: bool, write_csv: bool, out_dir: str):\n",
    "    \"\"\"This is a enhanced version of https://gist.github.com/ptschandl/ef67bbaa93ec67aba2cab0a7af47700b\n",
    "    This script exctracts variables from all logs from tensorflow event files (\"event*\"),\n",
    "    writes them to Pandas and finally stores them a csv-file or pickle-file including all (readable) runs of the logging directory.\n",
    "    Example usage:\n",
    "    # create csv file from all tensorflow logs in provided directory (.) and write it to folder \"./converted\"\n",
    "    tflogs2pandas.py . --csv --no-pkl --o converted\n",
    "    # creaste csv file from tensorflow logfile only and write into and write it to folder \"./converted\"\n",
    "    tflogs2pandas.py tflog.hostname.12345 --csv --no-pkl --o converted\n",
    "    \"\"\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    if os.path.isfile(logfile):\n",
    "        event_paths = [logfile]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"input argument {} has to be a file\".format(\n",
    "                logfile\n",
    "            )\n",
    "        )\n",
    "    # Call & append\n",
    "    if event_paths:\n",
    "        pp.pprint(\"Found tensorflow logs to process:\")\n",
    "        pp.pprint(event_paths)\n",
    "        all_logs = many_logs2pandas(event_paths)\n",
    "        pp.pprint(\"Head of created dataframe\")\n",
    "        pp.pprint(all_logs.head())\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        run_id = logfile.split(\".\")[5]+logfile.split(\".\")[6]\n",
    "        mode = logfile.split(\"/\")[-1].split(\"\\\\\")[1]\n",
    "        datetime = logfile.split(\"/\")[-1].split(\"\\\\\")[2]\n",
    "        if write_csv:\n",
    "            print(\"saving to csv file\")\n",
    "            out_file = os.path.join(out_dir, \"run_\"+mode+\"_\"+datetime+\"_\"+run_id+\"_file.csv\")\n",
    "            print(out_file)\n",
    "            all_logs.to_csv(out_file, index=None)\n",
    "        if write_pkl:\n",
    "            print(\"saving to pickle file\")\n",
    "            out_file = os.path.join(out_dir, \"all_training_logs_in_one_file.pkl\")\n",
    "            print(out_file)\n",
    "            all_logs.to_pickle(out_file)\n",
    "    else:\n",
    "        print(\"No event paths have been found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vzero: 3 files:\n",
      "../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\Random\\2019-11-14-210942-1-1\\summary\\tb\\events.out.tfevents.1573783803.gra1088\n",
      "../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\Random\\2019-11-14-210950-0-0\\summary\\tb\\events.out.tfevents.1573783811.gra1087\n",
      "../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\Random\\2019-11-14-210951-2-0\\summary\\tb\\events.out.tfevents.1573783814.gra1091\n",
      "vpoint1: 0 files:\n",
      "vpoint3: 0 files:\n"
     ]
    }
   ],
   "source": [
    "# folders = [\"point125\",\"point25\",\"point5\",\"point75\", \"one\"]\n",
    "folders = [\"vzero\",\"vpoint1\",\"vpoint3\"]\n",
    "# folders = [\"ddpg\"]\n",
    "event_files = dict()\n",
    "for folder in folders:\n",
    "    \n",
    "    event_files[folder] = glob.glob(\"../SHARCNET/Results/multi/lazy_visitor_sq_action/new/\"+folder+\"/**/events*\", recursive=True)\n",
    "    event_files[folder].sort()\n",
    "    print(folder+\": {} files:\".format(len(event_files[folder])))\n",
    "    for f in event_files[folder]:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\\\Random\\\\2019-11-14-210942-1-1\\\\summary\\\\tb\\\\events.out.tfevents.1573783803.gra1088']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1       24.516268\n",
      "2       39.996166\n",
      "3       32.525730\n",
      "4       43.945702\n",
      "5       30.289572\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/multi/lazy_visitor_sq_action/new/vzero\\run_Random_2019-11-14-210942-1-1_1573783803gra1088_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\\\Random\\\\2019-11-14-210950-0-0\\\\summary\\\\tb\\\\events.out.tfevents.1573783811.gra1087']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1       17.840870\n",
      "2       25.261267\n",
      "3       27.366371\n",
      "4       29.097244\n",
      "5       29.193121\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/multi/lazy_visitor_sq_action/new/vzero\\run_Random_2019-11-14-210950-0-0_1573783811gra1087_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/multi/lazy_visitor_sq_action/new/vzero\\\\Random\\\\2019-11-14-210951-2-0\\\\summary\\\\tb\\\\events.out.tfevents.1573783814.gra1091']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1       24.152161\n",
      "2       44.066761\n",
      "3       37.753838\n",
      "4       31.952959\n",
      "5       32.592751\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/multi/lazy_visitor_sq_action/new/vzero\\run_Random_2019-11-14-210951-2-0_1573783814gra1091_file.csv\n"
     ]
    }
   ],
   "source": [
    "out_root_dir = \"../SHARCNET/Results/Tensorboard_to_CSV/multi/lazy_visitor_sq_action/new/\"\n",
    "for folder, files in event_files.items():\n",
    "    out_dir = out_root_dir+folder\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    for f in files:\n",
    "        extract_tf_to_csv(logfile=f, write_pkl=False, write_csv=True, out_dir=out_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

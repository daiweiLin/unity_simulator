{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# source:\n",
    "# https://github.com/theRealSuperMario/supermariopy/blob/master/scripts/tflogs2pandas.py\n",
    "#########################################\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import traceback\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import click\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Extraction function\n",
    "def tflog2pandas(path: str) -> pd.DataFrame:\n",
    "    \"\"\"convert single tensorflow log file to pandas DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        path to tensorflow log file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        converted dataframe\n",
    "    \"\"\"\n",
    "    DEFAULT_SIZE_GUIDANCE = {\n",
    "        \"compressedHistograms\": 1,\n",
    "        \"images\": 1,\n",
    "        \"scalars\": 0,  # 0 means load all\n",
    "        \"histograms\": 1,\n",
    "    }\n",
    "    runlog_data = pd.DataFrame()\n",
    "    try:\n",
    "        event_acc = EventAccumulator(path, DEFAULT_SIZE_GUIDANCE)\n",
    "        event_acc.Reload()\n",
    "        tags = event_acc.Tags()[\"scalars\"]\n",
    "        for tag in tags:\n",
    "            if tag == \"rollout/return\" or tag == \"rollout/Q_mean\": #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Made change here\n",
    "                event_list = event_acc.Scalars(tag)\n",
    "                values = list(map(lambda x: x.value, event_list))\n",
    "                step = list(map(lambda x: x.step, event_list))\n",
    "                r = pd.Series(data=values, index=step, name=tag)\n",
    "                runlog_data = pd.concat([runlog_data, r], axis=1)\n",
    "    # Dirty catch of DataLossError\n",
    "    except:\n",
    "        print(\"Event file possibly corrupt: {}\".format(path))\n",
    "        traceback.print_exc()\n",
    "    return runlog_data\n",
    "\n",
    "\n",
    "def many_logs2pandas(event_paths):\n",
    "    all_logs = pd.DataFrame()\n",
    "    for path in event_paths:\n",
    "        log = tflog2pandas(path)\n",
    "        if log is not None:\n",
    "            if all_logs.shape[0] == 0:\n",
    "                all_logs = log\n",
    "            else:\n",
    "                all_logs = all_logs.append(log, ignore_index=True)\n",
    "    return all_logs\n",
    "\n",
    "\n",
    "def extract_tf_to_csv(logfile: str, write_pkl: bool, write_csv: bool, out_dir: str):\n",
    "    \"\"\"This is a enhanced version of https://gist.github.com/ptschandl/ef67bbaa93ec67aba2cab0a7af47700b\n",
    "    This script exctracts variables from all logs from tensorflow event files (\"event*\"),\n",
    "    writes them to Pandas and finally stores them a csv-file or pickle-file including all (readable) runs of the logging directory.\n",
    "    Example usage:\n",
    "    # create csv file from all tensorflow logs in provided directory (.) and write it to folder \"./converted\"\n",
    "    tflogs2pandas.py . --csv --no-pkl --o converted\n",
    "    # creaste csv file from tensorflow logfile only and write into and write it to folder \"./converted\"\n",
    "    tflogs2pandas.py tflog.hostname.12345 --csv --no-pkl --o converted\n",
    "    \"\"\"\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    if os.path.isfile(logfile):\n",
    "        event_paths = [logfile]\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"input argument {} has to be a file\".format(\n",
    "                logfile\n",
    "            )\n",
    "        )\n",
    "    # Call & append\n",
    "    if event_paths:\n",
    "        pp.pprint(\"Found tensorflow logs to process:\")\n",
    "        pp.pprint(event_paths)\n",
    "        all_logs = many_logs2pandas(event_paths)\n",
    "        pp.pprint(\"Head of created dataframe\")\n",
    "        pp.pprint(all_logs.head())\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        run_id = logfile.split(\".\")[5]+logfile.split(\".\")[6]\n",
    "        mode = logfile.split(\"/\")[-1].split(\"\\\\\")[1]\n",
    "        datetime = logfile.split(\"/\")[-1].split(\"\\\\\")[2]\n",
    "        if write_csv:\n",
    "            print(\"saving to csv file\")\n",
    "            out_file = os.path.join(out_dir, \"run_\"+mode+\"_\"+datetime+\"_\"+run_id+\"_file.csv\")\n",
    "            print(out_file)\n",
    "            all_logs.to_csv(out_file, index=None)\n",
    "        if write_pkl:\n",
    "            print(\"saving to pickle file\")\n",
    "            out_file = os.path.join(out_dir, \"all_training_logs_in_one_file.pkl\")\n",
    "            print(out_file)\n",
    "            all_logs.to_pickle(out_file)\n",
    "    else:\n",
    "        print(\"No event paths have been found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point125: 2 files:\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point125\\Random\\2019-10-28-144314-0-0\\summary\\tb\\events.out.tfevents.1572288214.gra134\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point125\\Random\\2019-10-28-144317-1-0\\summary\\tb\\events.out.tfevents.1572288217.gra133\n",
      "point25: 2 files:\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point25\\Random\\2019-10-28-144918-0-10\\summary\\tb\\events.out.tfevents.1572288579.gra1141\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point25\\Random\\2019-10-28-144918-1-10\\summary\\tb\\events.out.tfevents.1572288578.gra7\n",
      "point5: 0 files:\n",
      "point75: 2 files:\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point75\\Random\\2019-10-28-145507-0-30\\summary\\tb\\events.out.tfevents.1572288927.gra1142\n",
      "../SHARCNET/Results/single/ddpg/64_64_NN/new/point75\\Random\\2019-10-28-145508-1-30\\summary\\tb\\events.out.tfevents.1572288928.gra701\n"
     ]
    }
   ],
   "source": [
    "folders = [\"point125\",\"point25\",\"point5\",\"point75\"]\n",
    "# folders = [\"vzero\",\"vpoint1\",\"vpoint3\"]\n",
    "# folders = [\"ddpg\"]\n",
    "event_files = dict()\n",
    "for folder in folders:\n",
    "    \n",
    "    event_files[folder] = glob.glob(\"../SHARCNET/Results/single/ddpg/64_64_NN/new/\"+folder+\"/**/events*\", recursive=True)\n",
    "    event_files[folder].sort()\n",
    "    print(folder+\": {} files:\".format(len(event_files[folder])))\n",
    "    for f in event_files[folder]:\n",
    "            print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point125\\\\Random\\\\2019-10-28-144314-0-0\\\\summary\\\\tb\\\\events.out.tfevents.1572288214.gra134']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        5.778702\n",
      "2       20.894213\n",
      "3        4.486615\n",
      "4        3.466223\n",
      "5        7.057436\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point125\\run_Random_2019-10-28-144314-0-0_1572288214gra134_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point125\\\\Random\\\\2019-10-28-144317-1-0\\\\summary\\\\tb\\\\events.out.tfevents.1572288217.gra133']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        3.918313\n",
      "2        5.257696\n",
      "3       10.429074\n",
      "4        8.503400\n",
      "5        7.806859\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point125\\run_Random_2019-10-28-144317-1-0_1572288217gra133_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point25\\\\Random\\\\2019-10-28-144918-0-10\\\\summary\\\\tb\\\\events.out.tfevents.1572288579.gra1141']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        4.831378\n",
      "2        5.754683\n",
      "3       10.292846\n",
      "4       11.494301\n",
      "5        5.506523\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point25\\run_Random_2019-10-28-144918-0-10_1572288579gra1141_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point25\\\\Random\\\\2019-10-28-144918-1-10\\\\summary\\\\tb\\\\events.out.tfevents.1572288578.gra7']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        4.552157\n",
      "2        6.696897\n",
      "3        6.407269\n",
      "4        5.071745\n",
      "5       11.144839\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point25\\run_Random_2019-10-28-144918-1-10_1572288578gra7_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point75\\\\Random\\\\2019-10-28-145507-0-30\\\\summary\\\\tb\\\\events.out.tfevents.1572288927.gra1142']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        4.515109\n",
      "2        3.353491\n",
      "3       14.034638\n",
      "4        6.181571\n",
      "5        9.882821\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point75\\run_Random_2019-10-28-145507-0-30_1572288927gra1142_file.csv\n",
      "'Found tensorflow logs to process:'\n",
      "[   '../SHARCNET/Results/single/ddpg/64_64_NN/new/point75\\\\Random\\\\2019-10-28-145508-1-30\\\\summary\\\\tb\\\\events.out.tfevents.1572288928.gra701']\n",
      "'Head of created dataframe'\n",
      "   rollout/return\n",
      "1        2.758078\n",
      "2        7.509033\n",
      "3        8.426976\n",
      "4        5.484399\n",
      "5        8.927257\n",
      "saving to csv file\n",
      "../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/point75\\run_Random_2019-10-28-145508-1-30_1572288928gra701_file.csv\n"
     ]
    }
   ],
   "source": [
    "out_root_dir = \"../SHARCNET/Results/Tensorboard_to_CSV/single/ddpg/64_64_NN/new/\"\n",
    "for folder, files in event_files.items():\n",
    "    out_dir = out_root_dir+folder\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    for f in files:\n",
    "        extract_tf_to_csv(logfile=f, write_pkl=False, write_csv=True, out_dir=out_dir)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
